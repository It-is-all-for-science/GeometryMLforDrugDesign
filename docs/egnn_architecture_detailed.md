# Детальная архитектура EGNN модели

## Оглавление
1. [Общая структура](#общая-структура)
2. [Конфигурация модели](#конфигурация-модели)
3. [Компоненты архитектуры](#компоненты-архитектуры)
4. [Математическое описание](#математическое-описание)
5. [Параметры и размерности](#параметры-и-размерности)

---

## Общая структура

EGNN (E(n) Equivariant Graph Neural Network) — это геометрическая нейронная сеть, которая сохраняет эквивариантность к вращениям и трансляциям в 3D пространстве. Это критически важно для молекулярного моделирования, так как физические свойства молекул не зависят от их ориентации в пространстве.

### Архитектура модели состоит из трех основных частей:

```
Входные данные (атомы молекулы)
    ↓
[1] Node Embedding Layer (эмбеддинг атомных признаков)
    ↓
[2] EGNN Layers × N (геометрические слои с message passing)
    ↓
[3] Global Pooling + Output MLP (агрегация и предсказание)
    ↓
Выходное значение (HOMO-LUMO gap)
```

---

## Конфигурация модели

### EGNNConfig — параметры итоговой модели

```python
hidden_dim: 128          # Размерность скрытых представлений
num_layers: 4            # Количество EGNN слоев
output_dim: 1            # Размерность выхода (1 для регрессии gap)
node_feature_dim: 11     # Количество атомных признаков
edge_feature_dim: 0      # Признаки связей (не используются)
attention: True          # Механизм внимания включен
normalize: True          # Нормализация координат
tanh: True              # Tanh для стабилизации координат
dropout: 0.1            # Dropout для регуляризации
activation: 'swish'     # Функция активации (SiLU)
update_coords: False    # Координаты НЕ обновляются (для стабильности)
```

**Почему эти параметры?**
- `hidden_dim=128`: Достаточно для захвата сложных молекулярных паттернов, но не слишком большой для переобучения
- `num_layers=4`: Оптимальный баланс между глубиной сети и вычислительной эффективностью
- `attention=True`: Позволяет модели фокусироваться на важных атомных взаимодействиях
- `update_coords=False`: Отключено для стабильности обучения (координаты фиксированы)
- `activation='swish'`: Более гладкая активация, чем ReLU, лучше для градиентов

---

## Компоненты архитектуры

### 1. Node Embedding Layer

**Назначение:** Преобразует исходные атомные признаки в высокоразмерное представление.

**Структура:**
```python
nn.Sequential(
    nn.Linear(11, 128),    # 11 атомных признаков → 128 скрытых
    nn.ReLU(),             # Нелинейность
    nn.Dropout(0.1)        # Регуляризация
)
```

**Входные признаки (11 штук):**
1. Атомный номер (Z)
2. Степень узла (количество связей)
3. Формальный заряд
4. Количество водородов
5. Гибридизация (sp, sp2, sp3)
6. Ароматичность
7. Принадлежность к кольцу
8-11. Дополнительные химические свойства

**Выход:** Тензор размера `[N_atoms, 128]`

---

### 2. EGNN Layer (основной строительный блок)

Каждый EGNN слой состоит из нескольких подкомпонентов:

#### 2.1 Edge MLP (вычисление сообщений между атомами)

**Назначение:** Вычисляет, какую информацию один атом передает другому.

**Вход:**
- `h_i`: признаки атома-получателя `[E, 128]`
- `h_j`: признаки атома-отправителя `[E, 128]`
- `||r_ij||`: расстояние между атомами `[E, 1]`

**Структура:**
```python
nn.Sequential(
    nn.Linear(128 + 128 + 1, 128),  # Конкатенация h_i, h_j, distance
    nn.SiLU(),                       # Swish активация
    nn.Dropout(0.1),
    nn.Linear(128, 128),
    nn.SiLU()
)
```

**Выход:** Сообщения `[E, 128]` где E — количество связей

**Почему это важно:** Расстояние `||r_ij||` — это инвариантный признак (не меняется при вращении), что обеспечивает геометрическую корректность.

#### 2.2 Attention Mechanism (механизм внимания)

**Назначение:** Определяет важность каждого сообщения.

**Структура:**
```python
nn.Sequential(
    nn.Linear(128, 1),     # Сообщение → скаляр важности
    nn.Sigmoid()           # Вес в диапазоне [0, 1]
)
```

**Применение:**
```python
attention_weights = attention_mlp(messages)  # [E, 1]
messages = messages * attention_weights      # Взвешивание
```

**Интуиция:** Не все атомные взаимодействия одинаково важны. Например, связь C-C в бензольном кольце важнее, чем далекий атом водорода.

#### 2.3 Node MLP (обновление атомных признаков)

**Назначение:** Обновляет представление каждого атома на основе полученных сообщений.

**Вход:**
- `h_old`: старые признаки атома `[N, 128]`
- `messages_aggregated`: суммированные сообщения от соседей `[N, 128]`

**Структура:**
```python
nn.Sequential(
    nn.Linear(128 + 128, 128),  # Конкатенация старых признаков и сообщений
    nn.SiLU(),
    nn.Dropout(0.1),
    nn.Linear(128, 128)
)
```

**Residual Connection:**
```python
h_new = h_old + node_mlp([h_old, messages])  # Skip connection
h_new = LayerNorm(h_new)                      # Нормализация
```

**Почему residual:** Помогает градиентам проходить через глубокую сеть, предотвращает затухание градиентов.

#### 2.4 Coordinate MLP (обновление координат — ОТКЛЮЧЕНО)

**Назначение:** Теоретически может обновлять 3D координаты атомов эквивариантным образом.

**В нашей модели:** `update_coords=False` — координаты НЕ обновляются.

**Почему отключено:**
- Координаты из квантовых расчетов уже оптимальны
- Обновление координат может дестабилизировать обучение
- Для предсказания HOMO-LUMO gap достаточно фиксированной геометрии

**Если бы было включено:**
```python
# Вычисление направлений между атомами
directions = (pos_i - pos_j) / ||pos_i - pos_j||  # [E, 3]

# Веса обновлений
weights = coord_mlp([h_i, h_j, ||r_ij||])  # [E, 1]

# Эквивариантное обновление
pos_update = sum(weights * directions)  # [N, 3]
```

---

### 3. Global Pooling + Output MLP

#### 3.1 Центрирование координат

**Назначение:** Обеспечивает трансляционную инвариантность.

```python
pos_centered = pos - mean(pos)  # Центр масс в начало координат
```

**Почему:** Молекула в точке (0,0,0) и в точке (100,100,100) должна иметь одинаковые свойства.

#### 3.2 Global Mean Pooling

**Назначение:** Агрегирует информацию со всех атомов в одно представление молекулы.

```python
h_molecule = mean(h_atoms)  # [N, 128] → [1, 128]
```

**Альтернативы:**
- `sum`: Суммирование (зависит от размера молекулы)
- `max`: Максимум (теряет информацию)
- `mean`: Среднее (наш выбор — инвариантно к размеру)

#### 3.3 Output MLP

**Назначение:** Финальное предсказание HOMO-LUMO gap.

**Структура:**
```python
nn.Sequential(
    nn.Linear(128, 128),   # Промежуточный слой
    nn.ReLU(),
    nn.Dropout(0.1),
    nn.Linear(128, 1)      # Финальное предсказание
)
```

**Выход:** Скаляр — предсказанное значение HOMO-LUMO gap в эВ.

---

## Математическое описание

### Message Passing в EGNN Layer

**Шаг 1: Вычисление сообщений**
```
m_ij = φ_e(h_i, h_j, ||x_i - x_j||²)
```
где:
- `φ_e` — Edge MLP
- `h_i, h_j` — признаки атомов
- `||x_i - x_j||²` — квадрат расстояния (инвариант)

**Шаг 2: Attention (опционально)**
```
α_ij = σ(ψ(m_ij))
m_ij = α_ij · m_ij
```
где:
- `ψ` — Attention MLP
- `σ` — Sigmoid
- `α_ij` — вес важности связи

**Шаг 3: Агрегация сообщений**
```
m_i = Σ_j m_ij
```
Суммирование по всем соседям атома i

**Шаг 4: Обновление узлов**
```
h_i^(l+1) = h_i^(l) + φ_h(h_i^(l), m_i)
```
где:
- `φ_h` — Node MLP
- Residual connection: `h_i^(l) +`

**Шаг 5: Нормализация**
```
h_i^(l+1) = LayerNorm(h_i^(l+1))
```

### Финальное предсказание

```
h_graph = mean({h_i | i ∈ atoms})
y_pred = MLP_out(h_graph)
```

---

## Параметры и размерности

### Количество параметров по компонентам

**1. Node Embedding:**
```
Linear(11 → 128): 11 × 128 + 128 = 1,536 параметров
```

**2. Один EGNN Layer:**

Edge MLP:
```
Linear(257 → 128): 257 × 128 + 128 = 33,024
Linear(128 → 128): 128 × 128 + 128 = 16,512
```

Attention MLP:
```
Linear(128 → 1): 128 × 1 + 1 = 129
```

Node MLP:
```
Linear(256 → 128): 256 × 128 + 128 = 32,896
Linear(128 → 128): 128 × 128 + 128 = 16,512
```

Coordinate MLP (не используется, но есть):
```
Linear(257 → 128): 257 × 128 + 128 = 33,024
Linear(128 → 1): 128 × 1 + 1 = 129
```

LayerNorm:
```
2 × 128 = 256 (γ и β параметры)
```

**Итого на один EGNN Layer:** ~132,482 параметра

**3. Output MLP:**
```
Linear(128 → 128): 128 × 128 + 128 = 16,512
Linear(128 → 1): 128 × 1 + 1 = 129
```

### Общее количество параметров

```
Node Embedding:        1,536
EGNN Layers × 4:     529,928
Output MLP:           16,641
─────────────────────────────
ИТОГО:              ~548,105 параметров
```

### Размерности тензоров в процессе forward pass

Пример для молекулы с 20 атомами и 40 связями:

```
Вход:
  x:          [20, 11]      # Атомные признаки
  pos:        [20, 3]       # 3D координаты
  edge_index: [2, 40]       # Индексы связей

После Node Embedding:
  h:          [20, 128]     # Эмбеддинги атомов

В каждом EGNN Layer:
  messages:   [40, 128]     # Сообщения по связям
  h_updated:  [20, 128]     # Обновленные признаки атомов
  pos:        [20, 3]       # Координаты (неизменны)

После Global Pooling:
  h_global:   [1, 128]      # Представление молекулы

Выход:
  prediction: [1, 1]        # HOMO-LUMO gap
```

---

## Ключевые особенности реализации

### 1. Эквивариантность к вращениям

**Как достигается:**
- Используются только инвариантные признаки: расстояния `||r_ij||`
- Координаты не обновляются напрямую
- Центрирование координат перед обработкой

**Проверка:**
```python
# Если повернуть молекулу на произвольный угол
pos_rotated = rotation_matrix @ pos
# Предсказание не изменится
assert model(x, pos) == model(x, pos_rotated)
```

### 2. Инвариантность к трансляциям

**Как достигается:**
- Центрирование координат: `pos = pos - mean(pos)`
- Используются только относительные расстояния

**Проверка:**
```python
# Если сдвинуть молекулу в пространстве
pos_translated = pos + [100, 100, 100]
# Предсказание не изменится
assert model(x, pos) == model(x, pos_translated)
```

### 3. Стабильность обучения

**Техники:**
- Residual connections в каждом слое
- Layer Normalization после обновлений
- Dropout (0.1) для регуляризации
- Swish активация (более гладкая, чем ReLU)
- Отключение обновления координат

### 4. Эффективность

**Оптимизации:**
- Message passing только по существующим связям (не все пары атомов)
- Attention механизм фокусирует вычисления на важных взаимодействиях
- Батчевая обработка молекул разного размера

---

## Сравнение с другими архитектурами

| Характеристика | EGNN | GCN | FCNN |
|----------------|------|-----|------|
| Использует 3D координаты | ✅ Да | ❌ Нет | ❌ Нет |
| Эквивариантность | ✅ Да | ❌ Нет | ❌ Нет |
| Message passing | ✅ Да | ✅ Да | ❌ Нет |
| Attention | ✅ Да | ❌ Нет | ❌ Нет |
| Параметры | ~548K | ~180K | ~90K |
| MAE на тесте | 0.267 eV | 0.389 eV | 0.412 eV |

**Почему EGNN лучше:**
- Учитывает 3D геометрию молекул
- Сохраняет физические симметрии
- Attention фокусируется на важных взаимодействиях
- Более выразительные представления

---

## Практическое применение

### Обучение модели

```python
from src.step_03_models.egnn import create_egnn_model, EGNNConfig

# Создание модели
config = EGNNConfig(
    hidden_dim=128,
    num_layers=4,
    attention=True,
    dropout=0.1
)
model = create_egnn_model(config)

# Обучение
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

for epoch in range(100):
    for batch in dataloader:
        output = model(
            x=batch.x,
            pos=batch.pos,
            edge_index=batch.edge_index,
            batch=batch.batch
        )
        loss = criterion(output['prediction'], batch.y)
        loss.backward()
        optimizer.step()
```

### Предсказание для новой молекулы

```python
# Загрузка обученной модели
model.load_state_dict(torch.load('best_model.pt'))
model.eval()

# Предсказание
with torch.no_grad():
    result = model(x, pos, edge_index)
    gap_prediction = result['prediction'].item()
    print(f"Predicted HOMO-LUMO gap: {gap_prediction:.3f} eV")
```

---

## Заключение

EGNN модель — это современная геометрическая нейронная сеть, специально разработанная для молекулярного моделирования. Ее ключевые преимущества:

1. **Физическая корректность:** Сохраняет симметрии молекул
2. **Выразительность:** Attention и message passing захватывают сложные взаимодействия
3. **Стабильность:** Residual connections и нормализация обеспечивают надежное обучение
4. **Точность:** MAE 0.267 eV — на 31.3% лучше базовой версии

Эта архитектура представляет собой state-of-the-art подход к предсказанию молекулярных свойств и может быть адаптирована для других задач геометрического машинного обучения.
